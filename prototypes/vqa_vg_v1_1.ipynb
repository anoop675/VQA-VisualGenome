{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed"
      ],
      "metadata": {
        "id": "yfh53vz4_2jH"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Constants, URL and Path Definitions**\n"
      ],
      "metadata": {
        "id": "afNKinC3Bvlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "ROOT = \"./visual_genome_data\" # directory where the JSON dumps will be placed\n",
        "IMG_DIR = \"./vg_images\" # directory where images downloaded for experiments are stored\n",
        "\n",
        "# Files contains all fused embeddings for train, validation and test sets\n",
        "TRAIN_EMB = \"train_seq_embeddings_vg.pt\"\n",
        "VAL_EMB = \"val_seq_embeddings_vg.pt\"\n",
        "TEST_EMB = \"test_seq_embeddings_vg.pt\"\n",
        "\n",
        "VISUAL_GENOME_BASE = \"https://homes.cs.washington.edu/~ranjay/visualgenome/data/dataset\"\n",
        "QUESTION_ANS_ZIP = \"question_answers.json.zip\"\n",
        "IMAGE_DATA_ZIP = \"image_data.json.zip\"\n",
        "\n",
        "SEED = 42 #for reproducibility\n",
        "BATCH_EMB_BUILD = 96  # building embeddings batch (lower to avoid OOM)\n",
        "TRAIN_BATCH = 8\n",
        "VAL_BATCH = 16\n",
        "EPOCHS = 4\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "AJe2sEui_ycF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f264eec8-7436-456b-a9bc-c73a4651c683"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7fb9d912d0d0>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class (Embeddings)\n",
        "class EmbeddingDataset(Dataset):\n",
        "\tdef __init__(self, path):\n",
        "\t\tdataset_metadata_dict = torch.load(path)\n",
        "\t\tself.text = dataset_metadata_dict[\"text\"]\n",
        "\t\tself.img  = dataset_metadata_dict[\"img\"]\n",
        "\t\tself.text_mask = dataset_metadata_dict.get(\"text_mask\", torch.ones(self.text.shape[0], self.text.shape[1], dtype=torch.long))\n",
        "\t\tself.img_mask = dataset_metadata_dict.get(\"img_mask\", torch.ones(self.img.shape[0], self.img.shape[1], dtype=torch.long))\n",
        "\t\tself.labels = dataset_metadata_dict[\"labels\"]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.labels)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn {\n",
        "\t\t\t\"text_embedding\": self.text[idx],   # (Nt, D_txt)\n",
        "\t\t\t\"image_embedding\": self.img[idx],   # (Ni, D_img)\n",
        "\t\t\t\"text_mask\": self.text_mask[idx],\n",
        "\t\t\t\"image_mask\": self.img_mask[idx],\n",
        "\t\t\t\"label\": self.labels[idx]\n",
        "\t\t}"
      ],
      "metadata": {
        "id": "pWX8d4MT_vRv"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Defining Cross Attention Fusion mechanism using text and image embeddings**"
      ],
      "metadata": {
        "id": "tpn7y1xBBOm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Attention Fusion Model\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "\tdef __init__(self, d_model, nhead=8, dim_ff=2048, dropout=0.1):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.mha = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "\t\tself.norm1 = nn.LayerNorm(d_model)\n",
        "\t\tself.ff = nn.Sequential(\n",
        "\t\t\tnn.Linear(d_model, dim_ff),\n",
        "\t\t\tnn.GELU(),\n",
        "\t\t\tnn.Dropout(dropout),\n",
        "\t\t\tnn.Linear(dim_ff, d_model)\n",
        "\t\t)\n",
        "\t\tself.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "\tdef forward(self, q, kv, kv_key_padding_mask=None):\n",
        "\t\tattn_out, _ = self.mha(query=q, key=kv, value=kv, key_padding_mask=kv_key_padding_mask)\n",
        "\t\tq = self.norm1(q + attn_out)\n",
        "\t\tq2 = self.ff(q)\n",
        "\t\treturn self.norm2(q + q2)\n",
        "\n",
        "class CrossAttentionFusionNetwork(nn.Module):\n",
        "\tdef __init__(self, d_img=768, d_txt=768, d=512, n_heads=8, num_answers=1000, num_cross_layers=2, dropout=0.3):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.P_img = nn.Linear(d_img, d)\n",
        "\t\tself.P_txt = nn.Linear(d_txt, d)\n",
        "\t\tself.cross_blocks = nn.ModuleList([CrossAttentionBlock(d_model=d, nhead=n_heads) for _ in range(num_cross_layers)])\n",
        "\t\tself.fc1 = nn.Linear(d, 256)\n",
        "\t\tself.bn1 = nn.BatchNorm1d(256)\n",
        "\t\tself.classifier = nn.Linear(256, num_answers)\n",
        "\t\tself.relu = nn.ReLU()\n",
        "\t\tself.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\tdef forward(self, image_embedding, text_embedding, image_mask=None, text_mask=None):\n",
        "\t\tI = self.P_img(image_embedding)  # [B, Ni, d]\n",
        "\t\tT = self.P_txt(text_embedding)   # [B, Nt, d]\n",
        "\t\tkv_mask = None\n",
        "\t\tif image_mask is not None:\n",
        "\t\t\tkv_mask = (image_mask == 0)\n",
        "\t\tTq = T\n",
        "\t\tfor blk in self.cross_blocks:\n",
        "\t\t\tTq = blk(Tq, I, kv_key_padding_mask=kv_mask)\n",
        "\t\tpooled = Tq[:, 0, :]  # assumes CLS at 0\n",
        "\t\tx = self.relu(self.fc1(pooled))\n",
        "\t\tx = self.bn1(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\tlogits = self.classifier(x)\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "qGu8H38p_qGP"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Utilities for Data preparation**"
      ],
      "metadata": {
        "id": "hyWnY08dBF5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "# Utility function to download and unzip the JSON\n",
        "def download_and_unzip_vg_jsons(target_dir=ROOT, timeout=60):\n",
        "\tos.makedirs(target_dir, exist_ok=True) # if root dir does't exist, create directory\n",
        "\n",
        "\tqa_zip_url = f\"{VISUAL_GENOME_BASE}/{QUESTION_ANS_ZIP}\" #url to the QA pairs (zip file)\n",
        "\timg_meta_zip_url = f\"{VISUAL_GENOME_BASE}/{IMAGE_DATA_ZIP}\" #url to the image metadata (zip file)\n",
        "\tqa_zip_path = os.path.join(target_dir, QUESTION_ANS_ZIP) # local target to the downloaded QA pirs (zip file)\n",
        "\timg_zip_path = os.path.join(target_dir, IMAGE_DATA_ZIP) # local target to the downloaded images metadata (zip file)\n",
        "\n",
        "\t# Helper function to download a file from a URL\n",
        "\tdef fetch(url, out_path):\n",
        "\t\tprint(\"Downloading ZIP file from: \", url)\n",
        "\t\tfetch_request = requests.get(url, timeout=timeout, stream=True) # send a GET request to download the file in streaming mode\n",
        "\n",
        "\t\tif fetch_request.status_code != 200:\n",
        "\t\t\traise RuntimeError(f\"Failed to download ZIP file at {url} with status {fetch_request.status_code}\")\n",
        "\n",
        "\t\twith open(out_path, \"wb\") as f:\n",
        "\t\t\tfor chunk in fetch_request.iter_content(chunk_size=8192): # write the file in chunks (to avoid loading everything into memory)\n",
        "\t\t\t\tif chunk:\n",
        "\t\t\t\t\tf.write(chunk) # only write on non-empty chunks\n",
        "\t\tprint(\"Saved ZIP file in path: \", out_path)\n",
        "\n",
        "\tif not os.path.exists(qa_zip_path):\n",
        "\t\tfetch(qa_zip_url, qa_zip_path)\n",
        "\telse:\n",
        "\t\tprint(\"Path to the QA pairs already exists in \", qa_zip_path, \"\\n Using QA pairs from path\")\n",
        "\n",
        "\tif not os.path.exists(img_zip_path):\n",
        "\t\tfetch(img_meta_zip_url, img_zip_path)\n",
        "\telse:\n",
        "\t\tprint(\"Path to the images metadata already exists in \", img_zip_path, \"\\n Using images metadata from path\")\n",
        "\n",
        "\t# Loop over both downloaded ZIP files to extract their contents\n",
        "\tfor z in (qa_zip_path, img_zip_path):\n",
        "\t\tprint(\"Unzipping ZIP file: \", z)\n",
        "\t\twith ZipFile(z, 'r') as zip_ref:\n",
        "\t\t\tzip_ref.extractall(target_dir)\n",
        "\n",
        "\tprint(\"Download & unzip done, JSON files saved in \", target_dir)\n",
        "\n",
        "# Function to load Visual Genome QA from local JSON into a flat pandas dataframe\n",
        "def load_vg_qa_local(json_path, limit=2000):\n",
        "\tif not os.path.exists(json_path):\n",
        "\t\traise FileNotFoundError(f\"File not found: {json_path}\")\n",
        "\n",
        "\twith open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "\t\tdata = json.load(f)\n",
        "\n",
        "\trecords = []\n",
        "\tfor item in data:\n",
        "\t\timage_id = item.get(\"id\", None)\n",
        "\t\tfor qa_pair in item.get(\"qas\", []):\n",
        "\t\t\trecords.append({\n",
        "\t\t\t\t\"image_id\": image_id,\n",
        "\t\t\t\t\"question\": str(qa_pair[\"question\"]).strip(),\n",
        "\t\t\t\t\"answer\": str(qa_pair[\"answer\"]).strip().lower()\n",
        "\t\t\t})\n",
        "\t\t\tif limit and len(records) >= limit:\n",
        "\t\t\t\tbreak\n",
        "\t\tif limit and len(records) >= limit:\n",
        "\t\t\tbreak\n",
        "\n",
        "\tdf = pd.DataFrame(records)\n",
        "\tprint(f\"Loaded {len(df)} QA pairs from Visual Genome\")\n",
        "\treturn df\n",
        "\n",
        "# Load image metadata mapping\n",
        "def load_vg_image_data_from_local(local_dir=ROOT):\n",
        "\tlocal_img_meta = None\n",
        "\tp = os.path.join(local_dir, \"image_data.json\")\n",
        "\n",
        "\tif os.path.exists(p):\n",
        "\t\tlocal_img_meta = p\n",
        "\n",
        "\tif local_img_meta is None:\n",
        "\t\tprint(\"No image_data.json found in\", local_dir, \", returing empty metadata\")\n",
        "\t\treturn {}\n",
        "\n",
        "\twith open(local_img_meta, \"r\", encoding=\"utf-8\") as f:\n",
        "\t\tdata = json.load(f)\n",
        "\n",
        "\tmapping = {}\n",
        "\tfor rec in data:\n",
        "\t\timage_id = int(rec.get(\"image_id\") or rec.get(\"id\"))\n",
        "\t\tmapping[image_id] = {\"url\": rec.get(\"url\"), \"width\": rec.get(\"width\"), \"height\": rec.get(\"height\")}\n",
        "\n",
        "\tprint(\"Loaded image metadata for\", len(mapping), \"images\")\n",
        "\treturn mapping\n",
        "\n",
        "def predownload_images_for_df(df, image_meta_map, out_dir=IMG_DIR, max_images=1000):\n",
        "\tos.makedirs(out_dir, exist_ok=True)\n",
        "\timage_ids = list(dict.fromkeys(df[\"image_id\"].tolist()))\n",
        "\tcnt = 0\n",
        "\n",
        "\tfor img_id in tqdm(image_ids, desc=\"predownloading images\"):\n",
        "\t\tif cnt >= max_images:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tmeta = image_meta_map.get(int(img_id))\n",
        "\n",
        "\t\tif not meta:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\turl = meta.get(\"url\")\n",
        "\t\tif not url:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tout_path = os.path.join(out_dir, f\"{int(img_id)}.jpg\")\n",
        "\t\tif os.path.exists(out_path):\n",
        "\t\t\tcnt += 1\n",
        "\t\t\tcontinue\n",
        "\t\t'''\n",
        "\t\ttry:\n",
        "\t\t\tr = requests.get(url, timeout=60) #60 seconds\n",
        "\n",
        "\t\t\tif r.status_code == 200:\n",
        "\t\t\t\twith open(out_path, \"wb\") as f:\n",
        "\t\t\t\t\tf.write(r.content)\n",
        "\t\t\t\tcnt += 1\n",
        "\t\texcept Exception:\n",
        "\t\t\tcontinue'''\n",
        "\n",
        "\t\tmax_attempts = 3  # Retry up to 3 times\n",
        "\t\tfor attempt in range(max_attempts):\n",
        "\t\t\t\ttry:\n",
        "\t\t\t\t\t\tr = requests.get(url, timeout=60, stream=True)  # Added stream=True\n",
        "\t\t\t\t\t\tif r.status_code == 200:\n",
        "\t\t\t\t\t\t\t\twith open(out_path, \"wb\") as f:\n",
        "\t\t\t\t\t\t\t\t\t\tfor chunk in r.iter_content(8192):  # Write in chunks\n",
        "\t\t\t\t\t\t\t\t\t\t\t\tif chunk:\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\tf.write(chunk)\n",
        "\t\t\t\t\t\t\t\tcnt += 1\n",
        "\t\t\t\t\t\t\t\tbreak  # Success, exit retry loop\n",
        "\t\t\t\texcept Exception:\n",
        "\t\t\t\t\t\ttime.sleep(2)  # Wait a bit before retrying\n",
        "\t\t\t\t\t\tif attempt == max_attempts - 1:\n",
        "\t\t\t\t\t\t\t\tprint(f\"Failed to download image {img_id} after {max_attempts} attempts.\")\n",
        "\n",
        "\tprint(\"Downloaded\", cnt, \"images to\", out_dir)\n",
        "\n",
        "def predownload_images_for_df_concurrently(df, image_meta_map, out_dir=IMG_DIR, max_images=1000, max_workers=32, timeout=60):\n",
        "    os.makedirs(out_dir, exist_ok=True)\n",
        "    # unique image ids, preserve order\n",
        "    image_ids = list(dict.fromkeys(df[\"image_id\"].tolist()))\n",
        "    cnt = 0\n",
        "    submitted = 0\n",
        "    futures = []\n",
        "\n",
        "    def download_one(url, out_path, img_id):\n",
        "        if os.path.exists(out_path):\n",
        "            return True\n",
        "        max_attempts = 3\n",
        "        for attempt in range(1, max_attempts + 1):\n",
        "            try:\n",
        "                r = requests.get(url, timeout=timeout, stream=True)\n",
        "                if r.status_code == 200:\n",
        "                    # write in chunks to avoid mem spikes\n",
        "                    with open(out_path, \"wb\") as f:\n",
        "                        for chunk in r.iter_content(chunk_size=8192):\n",
        "                            if chunk:\n",
        "                                f.write(chunk)\n",
        "                    return True\n",
        "                else:\n",
        "                    # non-200, no point retrying too many times quickly\n",
        "                    # small pause then retry\n",
        "                    time.sleep(0.5)\n",
        "            except Exception:\n",
        "                # backoff a bit\n",
        "                time.sleep(1.0 * attempt)\n",
        "        # failed after retries\n",
        "        return False\n",
        "\n",
        "    # Submit up to max_images tasks (skip IDs with no URL/meta)\n",
        "    with ThreadPoolExecutor(max_workers=max_workers) as exe:\n",
        "        for img_id in image_ids:\n",
        "            if submitted >= max_images:\n",
        "                break\n",
        "\n",
        "            meta = image_meta_map.get(int(img_id))\n",
        "            if not meta:\n",
        "                continue\n",
        "\n",
        "            url = meta.get(\"url\")\n",
        "            if not url:\n",
        "                continue\n",
        "\n",
        "            out_path = os.path.join(out_dir, f\"{int(img_id)}.jpg\")\n",
        "            # if file already exists, count it and don't submit a job\n",
        "            if os.path.exists(out_path):\n",
        "                cnt += 1\n",
        "                submitted += 1\n",
        "                continue\n",
        "\n",
        "            # submit download task\n",
        "            futures.append(exe.submit(download_one, url, out_path, img_id))\n",
        "            submitted += 1\n",
        "\n",
        "        # show progress and collect results\n",
        "        for f in tqdm(as_completed(futures), total=len(futures), desc=\"predownloading images\"):\n",
        "            try:\n",
        "                success = f.result()\n",
        "                if success:\n",
        "                    cnt += 1\n",
        "            except Exception as e:\n",
        "                # individual download failed (already retried inside)\n",
        "                # print or log optionally\n",
        "                # print(\"Download task error:\", e)\n",
        "                pass\n",
        "\n",
        "    print(\"Downloaded\", cnt, \"images to\", out_dir)"
      ],
      "metadata": {
        "id": "g6tae-i9_fFi"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering and Extraction (building and saving text embeddings and image embeddings from text and image backbones)**"
      ],
      "metadata": {
        "id": "HRGwV5bLAaq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Build sequence-level embeddings (BERT tokens and ViT patch tokens)\n",
        "# Tolerant: uses local IMG_DIR images first; if missing, tries image_meta_map URL (download)\n",
        "# ---------------------------\n",
        "def build_and_save_embeddings(df, tokenizer, img_preprocessor, text_encoder, img_encoder, image_meta_map, out_path,\n",
        "\t\t\t\t\t\t\t  local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD, max_text_len=64):\n",
        "\t\"\"\"\n",
        "\tBuilds and saves sequence-level embeddings with a fixed text token length.\n",
        "\n",
        "\t- text_seq: (N, max_text_len, D_txt)\n",
        "\t- text_mask: (N, max_text_len)\n",
        "\t- img_seq:  (N, Ni, D_img)  (Ni fixed for ViT)\n",
        "\t- img_mask: (N, Ni)\n",
        "\t\"\"\"\n",
        "\ttext_encoder.eval()\n",
        "\timg_encoder.eval()\n",
        "\ttext_encoder.to(device)\n",
        "\timg_encoder.to(device)\n",
        "\tos.makedirs(local_image_dir, exist_ok=True)\n",
        "\n",
        "\tall_text_seq = []\n",
        "\tall_img_seq = []\n",
        "\tall_text_masks = []\n",
        "\tall_img_masks = []\n",
        "\tall_labels = []\n",
        "\tdropped = 0\n",
        "\n",
        "\tfor i in tqdm(range(0, len(df), batch_size), desc=f\"Building {out_path}\"):\n",
        "\t\tbatch = df.iloc[i:i+batch_size]\n",
        "\t\tqs = batch[\"question\"].tolist()\n",
        "\t\timage_ids = batch[\"image_id\"].tolist()\n",
        "\n",
        "\t\timgs = []\n",
        "\t\tvalid_indices = []\n",
        "\t\tfor idx, img_id in enumerate(image_ids):\n",
        "\t\t\tlocal_path = os.path.join(local_image_dir, f\"{int(img_id)}.jpg\")\n",
        "\t\t\timg_path = None\n",
        "\t\t\tif os.path.exists(local_path):\n",
        "\t\t\t\timg_path = local_path\n",
        "\t\t\telse:\n",
        "\t\t\t\tmeta = image_meta_map.get(int(img_id), {})\n",
        "\t\t\t\turl = meta.get(\"url\")\n",
        "\t\t\t\tif url:\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t# try to download on demand\n",
        "\t\t\t\t\t\tr = requests.get(url, timeout=10)\n",
        "\t\t\t\t\t\tif r.status_code == 200:\n",
        "\t\t\t\t\t\t\twith open(local_path, \"wb\") as f:\n",
        "\t\t\t\t\t\t\t\tf.write(r.content)\n",
        "\t\t\t\t\t\t\timg_path = local_path\n",
        "\t\t\t\t\texcept Exception:\n",
        "\t\t\t\t\t\timg_path = None\n",
        "\t\t\tif not img_path or not os.path.exists(img_path):\n",
        "\t\t\t\tdropped += 1\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ttry:\n",
        "\t\t\t\timg = Image.open(img_path).convert(\"RGB\")\n",
        "\t\t\t\timgs.append(img)\n",
        "\t\t\t\tvalid_indices.append(idx)\n",
        "\t\t\texcept Exception:\n",
        "\t\t\t\tdropped += 1\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\tif len(imgs) == 0:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tbatch_qs = [qs[k] for k in valid_indices]\n",
        "\t\tbatch_labels = batch[\"label\"].values[valid_indices].tolist()\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# Tokenize with fixed max length so all batches have same Nt\n",
        "\t\t\ttokenized_qs = tokenizer(batch_qs, padding=\"max_length\", truncation=True, max_length=max_text_len, return_tensors=\"pt\")\n",
        "\t\t\ttokenized_qs = {k: v.to(device) for k, v in tokenized_qs.items()}\n",
        "\t\t\ttext_outputs = text_encoder(**tokenized_qs)\n",
        "\t\t\t# text_outputs.last_hidden_state -> (B, max_text_len, D_txt)\n",
        "\t\t\ttext_seq = text_outputs.last_hidden_state.detach().cpu()\n",
        "\t\t\ttext_mask = tokenized_qs[\"attention_mask\"].detach().cpu()  # (B, max_text_len)\n",
        "\n",
        "\t\t\t# Images -> ViT patch tokens (Ni fixed)\n",
        "\t\t\timg_inputs = img_preprocessor(images=imgs, return_tensors=\"pt\")\n",
        "\t\t\timg_inputs = {k: v.to(device) for k, v in img_inputs.items()}\n",
        "\t\t\timg_outputs = img_encoder(**img_inputs)\n",
        "\t\t\timg_seq = img_outputs.last_hidden_state.detach().cpu()  # (B, Ni, D_img)\n",
        "\t\t\timg_mask = torch.ones(img_seq.shape[0], img_seq.shape[1], dtype=torch.long)\n",
        "\n",
        "\t\tall_text_seq.append(text_seq)\n",
        "\t\tall_img_seq.append(img_seq)\n",
        "\t\tall_text_masks.append(text_mask)\n",
        "\t\tall_img_masks.append(img_mask)\n",
        "\t\tall_labels.append(torch.tensor(batch_labels, dtype=torch.long))\n",
        "\n",
        "\tif len(all_labels) == 0:\n",
        "\t\traise RuntimeError(\"No embeddings were created (no valid images found). Check local images or image_meta_map URLs.\")\n",
        "\n",
        "\t# Now concatenation will succeed because every text_seq has shape (B, max_text_len, D)\n",
        "\ttext_seq = torch.cat(all_text_seq, dim=0)\n",
        "\timg_seq = torch.cat(all_img_seq, dim=0)\n",
        "\ttext_masks = torch.cat(all_text_masks, dim=0)\n",
        "\timg_masks = torch.cat(all_img_masks, dim=0)\n",
        "\tlabels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "\ttorch.save({\"text\": text_seq, \"img\": img_seq, \"text_mask\": text_masks, \"img_mask\": img_masks, \"labels\": labels}, out_path)\n",
        "\tprint(\"Saved embeddings:\", out_path, \"Dropped samples during build:\", dropped)"
      ],
      "metadata": {
        "id": "nEj-yvFh_YWS"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preparation (Building Dataframe for Feature Engineering and Extraction)**"
      ],
      "metadata": {
        "id": "IKY8ohQg_8ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vg_dataframe(local_dir=ROOT, limit=None):\n",
        "\tqa_json = os.path.join(local_dir, \"question_answers.json\")\n",
        "\n",
        "\tif not os.path.exists(qa_json):\n",
        "\t\traise FileNotFoundError(f\"Please put 'question_answers.json' into {local_dir} or run download_and_unzip_vg_jsons().\")\n",
        "\n",
        "\tprint(\"Loading QA from\", qa_json)\n",
        "\tdf = load_vg_qa_local(qa_json, limit=limit)\n",
        "\tdf[\"question\"] = df[\"question\"].astype(str)\n",
        "\tdf[\"answer\"] = df[\"answer\"].astype(str).str.lower().str.strip()\n",
        "\tdf = df[df[\"question\"].str.len() > 0]\n",
        "\n",
        "\tif limit:\n",
        "\t\tdf = df.sample(n=min(limit, len(df)), random_state=SEED).reset_index(drop=True)\n",
        "\telse:\n",
        "\t\t#print(\"No limit defined, so doing a random split\")\n",
        "\t\t#df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\t\traise Exception(\"The limit on the number of QA pairs cannot be None\")\n",
        "\n",
        "\tn = len(df)\n",
        "\tn_train = int(0.8 * n)\n",
        "\tn_val = int(0.1 * n)\n",
        "\ttrain_df = df.iloc[:n_train].reset_index(drop=True)\n",
        "\tval_df = df.iloc[n_train:n_train + n_val].reset_index(drop=True)\n",
        "\ttest_df = df.iloc[n_train + n_val:].reset_index(drop=True)\n",
        "\tprint(\"Split sizes -> train:\", len(train_df), \"val:\", len(val_df), \"test:\", len(test_df))\n",
        "\n",
        "\treturn train_df, val_df, test_df"
      ],
      "metadata": {
        "id": "EWuAk5At-_zb"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluate and Train the Model**"
      ],
      "metadata": {
        "id": "2llSqdtr-xTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_dataloader, criterion, device=DEVICE):\n",
        "\tmodel.eval()\n",
        "\ttotal_val_loss = 0.0\n",
        "\tpredictions = []\n",
        "\ttrue_vals = []\n",
        "\tconf = []\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tfor batch in val_dataloader:\n",
        "\t\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t'image_embedding': batch['image_embedding'],\n",
        "\t\t\t\t'text_embedding': batch['text_embedding'],\n",
        "\t\t\t\t'image_mask': batch.get('image_mask', None),\n",
        "\t\t\t\t'text_mask': batch.get('text_mask', None)\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\toutputs = model(**inputs)\n",
        "\t\t\tlabels = batch['label']\n",
        "\t\t\tloss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "\t\t\ttotal_val_loss += loss.item()\n",
        "\t\t\tprobs = torch.max(outputs.softmax(dim=1), dim=-1)[0].detach().cpu().numpy()\n",
        "\t\t\toutputs = outputs.argmax(-1)\n",
        "\t\t\tpredictions.append(outputs.detach().cpu().numpy())\n",
        "\t\t\ttrue_vals.append(labels.cpu().numpy())\n",
        "\t\t\tconf.append(probs)\n",
        "\n",
        "\tloss_val_avg = total_val_loss / len(val_dataloader)\n",
        "\tpredictions = np.concatenate(predictions, axis=0)\n",
        "\ttrue_vals = np.concatenate(true_vals, axis=0)\n",
        "\tconf = np.concatenate(conf, axis=0)\n",
        "\n",
        "\treturn loss_val_avg, predictions, true_vals, conf\n",
        "\n",
        "def train_loop(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epochs=EPOCHS, device=DEVICE):\n",
        "\tbest_val_acc = 0.0\n",
        "\n",
        "\tfor epoch in range(1, epochs+1):\n",
        "\t\tmodel.train()\n",
        "\t\ttotal_train_loss = 0.0\n",
        "\t\ttrain_predictions = []\n",
        "\t\ttrain_true_vals = []\n",
        "\n",
        "\t\tfor batch in tqdm(train_dataloader, desc=f\"Train epoch {epoch}\"):\n",
        "\t\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t'image_embedding': batch['image_embedding'],\n",
        "\t\t\t\t'text_embedding': batch['text_embedding'],\n",
        "\t\t\t\t'image_mask': batch.get('image_mask', None),\n",
        "\t\t\t\t'text_mask': batch.get('text_mask', None)\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\tlabels = batch['label']\n",
        "\t\t\toutputs = model(**inputs)\n",
        "\t\t\tloss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tloss.backward()\n",
        "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tscheduler.step()\n",
        "\t\t\ttotal_train_loss += loss.item()\n",
        "\t\t\ttrain_predictions.append(outputs.argmax(-1).detach().cpu().numpy())\n",
        "\t\t\ttrain_true_vals.append(labels.cpu().numpy())\n",
        "\n",
        "\t\ttrain_preds = np.concatenate(train_predictions, axis=0)\n",
        "\t\ttrain_trues = np.concatenate(train_true_vals, axis=0)\n",
        "\t\ttrain_acc = accuracy_score(train_preds, train_trues)\n",
        "\t\tval_loss, val_preds, val_trues, _ = evaluate(model, val_dataloader, criterion, device=device)\n",
        "\t\tval_acc = accuracy_score(val_preds, val_trues)\n",
        "\t\tprint(f\"Epoch {epoch}: train_loss={total_train_loss/len(train_dataloader):.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "\t\tif val_acc > best_val_acc:\n",
        "\t\t\tbest_val_acc = val_acc\n",
        "\t\t\ttorch.save(model.state_dict(), \"best_model_vg_fusion.pth\")\n",
        "\t\t\tprint(\"Saved best model with val_acc:\", best_val_acc)\n",
        "\n",
        "\treturn best_val_acc\n"
      ],
      "metadata": {
        "id": "EX7m5jG6-lMD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**VQA Pipeline**"
      ],
      "metadata": {
        "id": "teg9tg46-bQ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MbuDZRCqfoUw"
      },
      "outputs": [],
      "source": [
        "def vqa_pipeline(limit_examples=2000, top_k=1000, max_images_to_download=1000, download_jsons=True): #limiting to only 2000 examples for experimenting (default), None for full dataset (appox 108000 examples)\n",
        "\tdef map_label(ans):\n",
        "\t\treturn ans_to_labels.get(ans, -1)\n",
        "\ttry:\n",
        "\t\t#--------------------------------------------------Data Preparation------------------------------------------------------------------\n",
        "\t\t# Downloading JSONS for extracting QA pairs and images metadata (set to False, if already downloaded locally)\n",
        "\t\tif download_jsons:\n",
        "\t\t\tprint(\"Downloading Visual Genome JSONs\")\n",
        "\t\t\tdownload_and_unzip_vg_jsons(ROOT)\n",
        "\n",
        "\t\t# Building dataframes (based on limit size)\n",
        "\t\ttrain_df, val_df, test_df = build_vg_dataframe(local_dir=ROOT, limit=limit_examples)\n",
        "\n",
        "\t\t# Load image meta mapping\n",
        "\t\timage_meta_map = load_vg_image_data_from_local(local_dir=ROOT)\n",
        "\n",
        "\t\t# Build top-K answer vocabulary (for classifying answers)\n",
        "\t\tall_answers = pd.concat([train_df[\"answer\"], val_df[\"answer\"], test_df[\"answer\"]])\n",
        "\t\tans_counts = Counter(all_answers.tolist())\n",
        "\t\tmost_common = [a for a, _ in ans_counts.most_common(top_k)]\n",
        "\t\tans_to_labels = {a: i for i, a in enumerate(most_common)}\n",
        "\t\tlabel_to_ans = {i: a for a, i in ans_to_labels.items()}\n",
        "\t\tprint(\"Top-K answer vocab size:\", len(ans_to_labels))\n",
        "\n",
        "\t\ttrain_df[\"label\"] = train_df[\"answer\"].apply(map_label)\n",
        "\t\tval_df[\"label\"] = val_df[\"answer\"].apply(map_label)\n",
        "\t\ttest_df[\"label\"] = test_df[\"answer\"].apply(map_label)\n",
        "\t\t# drop -1s\n",
        "\t\ttrain_df = train_df[train_df[\"label\"] >= 0].reset_index(drop=True)\n",
        "\t\tval_df = val_df[val_df[\"label\"] >= 0].reset_index(drop=True)\n",
        "\t\ttest_df = test_df[test_df[\"label\"] >= 0].reset_index(drop=True)\n",
        "\t\tprint(\"After mapping to top-K -> train:\", len(train_df), \"val:\", len(val_df), \"test:\", len(test_df))\n",
        "\n",
        "\t\t# Pre-download images subset for speed\n",
        "\t\tprint(\"Pre-downloading up to\", max_images_to_download, \"images for training/val/test\")\n",
        "\t\tcombined_df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
        "\t\tpredownload_images_for_df_concurrently(combined_df, image_meta_map, out_dir=IMG_DIR, max_images=max_images_to_download)\n",
        "\t\tprint(\"Data Preperation completed!\")\n",
        "\n",
        "    #--------------------------------------------------Feature Engineering------------------------------------------------------------------\n",
        "\t\t# Load backbones (frozen)\n",
        "\t\ttokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # loads BERT's tokenizer to tokenize the question strings into many lowercased tokens and convert them into token IDs that BERT understands\n",
        "\t\ttext_encoder_bert = AutoModel.from_pretrained(\"bert-base-uncased\") # loads a pre-trained BERT model used for converting the token IDs and produces dense vector embeddings for each token\n",
        "\t\tfor p in text_encoder_bert.parameters(): # leaving the parameters (tensors) of the BERT model unchanged (frozen) (to avoid re-training it when the entire fusion network learns)\n",
        "\t\t\tp.requires_grad = False # No need to compute gradients for each tensor during backpropagation\n",
        "\n",
        "\t\timg_preprocessor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\") # loads ViT’s feature extractor that normalizes & resizes images to 224×224 patches\n",
        "\t\timg_encoder_vit = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\") #loads the ViT model\n",
        "\t\tfor p in img_encoder_vit.parameters(): # leaving the parameters (tensors) of the ViT model unchanged (frozen) (to avoid re-training it when the entire fusion network learns)\n",
        "\t\t\tp.requires_grad = False ## No need to compute gradients for each tensor during backpropagation\n",
        "\n",
        "\t\t# Build embeddings files (sequence-level)\n",
        "\t\tif not os.path.exists(TRAIN_EMB):\n",
        "\t\t\tbuild_and_save_embeddings(train_df, tokenizer, img_preprocessor, text_encoder_bert, img_encoder_vit, image_meta_map, TRAIN_EMB, local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD)\n",
        "\t\tif not os.path.exists(VAL_EMB):\n",
        "\t\t\tbuild_and_save_embeddings(val_df, tokenizer, img_preprocessor, text_encoder_bert, img_encoder_vit, image_meta_map, VAL_EMB, local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD)\n",
        "\t\tif not os.path.exists(TEST_EMB):\n",
        "\t\t\tbuild_and_save_embeddings(test_df, tokenizer, img_preprocessor, text_encoder_bert, img_encoder_vit, image_meta_map, TEST_EMB, local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD)\n",
        "\t\tprint(\"Feature Engineering completed!\")\n",
        "\n",
        "    #-----------------------------------------------------Model Initialization-----------------------------------------------------------------\n",
        "\t\t# Datasets / Dataloaders\n",
        "\t\ttrain_dataset = EmbeddingDataset(TRAIN_EMB)\n",
        "\t\tval_dataset = EmbeddingDataset(VAL_EMB)\n",
        "\t\ttest_dataset = EmbeddingDataset(TEST_EMB)\n",
        "\t\ttrain_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\t\tval_dataloader = DataLoader(val_dataset, batch_size=VAL_BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\t\ttest_dataloader = DataLoader(test_dataset, batch_size=VAL_BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\t\t# Model and Training setup\n",
        "\t\tnum_answers_total = len(ans_to_labels)\n",
        "\t\tmodel = CrossAttentionFusionNetwork(d_img=768, d_txt=768, d=512, n_heads=8, num_answers=num_answers_total, num_cross_layers=2, dropout=0.3)\n",
        "\t\tmodel.to(DEVICE)\n",
        "\t\tcriterion = nn.CrossEntropyLoss()\n",
        "\t\ttrain_steps = len(train_dataloader) * EPOCHS\n",
        "\t\twarm_steps = max(1, int(train_steps * 0.1))\n",
        "\t\toptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5, eps=1e-8)\n",
        "\t\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warm_steps, num_training_steps=train_steps)\n",
        "\t\tprint(\"Model Initialization completed!\")\n",
        "\n",
        "    #-----------------------------------------------------Model Training + Testing-----------------------------------------------------------------\n",
        "\t\t# Train\n",
        "\t\tbest_val = train_loop(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epochs=EPOCHS, device=DEVICE)\n",
        "\t\tprint(\"Best val acc:\", best_val)\n",
        "\n",
        "\t\t# Test\n",
        "\t\tmodel.load_state_dict(torch.load(\"best_model_vg_fusion.pth\", map_location=DEVICE))\n",
        "\t\ttest_loss, preds, truths, conf = evaluate(model, test_dataloader, criterion, device=DEVICE)\n",
        "\t\ttest_acc, test_prec, test_recall, test_f1 = accuracy_score(preds, truths), precision_score(preds, truths, average='weighted'), \\\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\trecall_score(preds, truths, average='weighted'), f1_score(preds, truths, average='weighted')\n",
        "\t\tprint(\"Test loss:\", test_loss)\n",
        "\t\tprint(\"Test performance:\")\n",
        "\t\tprint(f\"Accuracy: {test_acc:.3f}, Precision: {test_prec:.3f}, Recall: {test_recall:.3f}, F1 Score: {test_f1:.3f}\")\n",
        "\n",
        "\texcept Exception as e:\n",
        "\t\tprint(\"Error in VQA pipeline due to error:\", e)\n",
        "\t\tprint(traceback.format_exc())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Execute the pipeline**"
      ],
      "metadata": {
        "id": "ZI1ze6PT-BD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "\t#!zip -r /content/vg_images.zip /content/vg_images\n",
        "\t#files.download(\"/content/vg_images.zip\")\n",
        "\n",
        "\t#!zip -r /content/visual_genome_data.zip /content/visual_genome_data\n",
        "\t#files.download(\"/content/visual_genome_data.zip\")\n",
        "\n",
        "\t# TODO: adjust params to scale up training\n",
        "\tvqa_pipeline(limit_examples=1000000, top_k=5000, max_images_to_download=1000000, download_jsons=True) #small params for quick experiments\n",
        "\n",
        "\n",
        "\t'''\n",
        "\tTODO: Extend the data preparation step to check if the vg_images.zip and visual_genome_data.zip\n",
        "\tare present in google drive, (in anoopsenthil13@gmail.com), if so unzip them and use the images and qa pairs from there\n",
        "\tif not present, then download using fetch request from the author's dataset repo\n",
        "\t'''"
      ],
      "metadata": {
        "id": "sE_VOJTU-JLX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        },
        "outputId": "1d78fcd9-7ab2-47f5-d41c-1c88ee43e074"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Visual Genome JSONs\n",
            "Path to the QA pairs already exists in  ./visual_genome_data/question_answers.json.zip \n",
            " Using QA pairs from path\n",
            "Path to the images metadata already exists in  ./visual_genome_data/image_data.json.zip \n",
            " Using images metadata from path\n",
            "Unzipping ZIP file:  ./visual_genome_data/question_answers.json.zip\n",
            "Unzipping ZIP file:  ./visual_genome_data/image_data.json.zip\n",
            "Download & unzip done, JSON files saved in  ./visual_genome_data\n",
            "Loading QA from ./visual_genome_data/question_answers.json\n",
            "Loaded 1000000 QA pairs from Visual Genome\n",
            "Split sizes -> train: 800000 val: 100000 test: 100000\n",
            "Loaded image metadata for 108077 images\n",
            "Top-K answer vocab size: 5000\n",
            "After mapping to top-K -> train: 590696 val: 73937 test: 73949\n",
            "Pre-downloading up to 1000000 images for training/val/test\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "predownloading images:   3%|▎         | 1405/47894 [01:04<35:47, 21.65it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2907769308.py\u001b[0m in \u001b[0;36mpredownload_images_for_df_concurrently\u001b[0;34m(df, image_meta_map, out_dir, max_images, max_workers, timeout)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;31m# show progress and collect results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mas_completed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfutures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"predownloading images\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mas_completed\u001b[0;34m(fs, timeout)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m             \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_timeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    654\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 655\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    656\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    354\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 355\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    356\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-975961865.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m# TODO: adjust params to scale up training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mvqa_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_images_to_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_jsons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#small params for quick experiments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-700607218.py\u001b[0m in \u001b[0;36mvqa_pipeline\u001b[0;34m(limit_examples, top_k, max_images_to_download, download_jsons)\u001b[0m\n\u001b[1;32m     35\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Pre-downloading up to\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_images_to_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"images for training/val/test\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m                 \u001b[0mcombined_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m                 \u001b[0mpredownload_images_for_df_concurrently\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcombined_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_meta_map\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mIMG_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_images\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_images_to_download\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Data Preperation completed!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2907769308.py\u001b[0m in \u001b[0;36mpredownload_images_for_df_concurrently\u001b[0;34m(df, image_meta_map, out_dir, max_images, max_workers, timeout)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0;31m# Submit up to max_images tasks (skip IDs with no URL/meta)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mThreadPoolExecutor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_workers\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mexe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_id\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mimage_ids\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0msubmitted\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mmax_images\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_tb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 647\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    648\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/concurrent/futures/thread.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, cancel_futures)\u001b[0m\n\u001b[1;32m    237\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_threads\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m                 \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m     \u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecutor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__doc__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1148\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1149\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1150\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1151\u001b[0m             \u001b[0;31m# the behavior of a negative timeout isn't documented, but\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1169\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1170\u001b[0m                 \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1171\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}