{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V6E1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoTokenizer, AutoModel, AutoFeatureExtractor\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import zipfile\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from zipfile import ZipFile\n",
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import traceback\n",
        "from PIL import Image\n",
        "from matplotlib import pyplot as plt"
      ],
      "metadata": {
        "id": "yfh53vz4_2jH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Constants, URL and Path Definitions**\n"
      ],
      "metadata": {
        "id": "afNKinC3Bvlh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "ROOT = \"./visual_genome_data\" # directory where the JSON dumps will be placed\n",
        "IMG_DIR = \"./vg_images\" # directory where images downloaded for experiments are stored\n",
        "\n",
        "# Files contains all fused embeddings for train, validation and test sets\n",
        "TRAIN_EMB = \"train_seq_embeddings_vg.pt\"\n",
        "VAL_EMB = \"val_seq_embeddings_vg.pt\"\n",
        "TEST_EMB = \"test_seq_embeddings_vg.pt\"\n",
        "\n",
        "VISUAL_GENOME_BASE = \"https://homes.cs.washington.edu/~ranjay/visualgenome/data/dataset\"\n",
        "QUESTION_ANS_ZIP = \"question_answers.json.zip\"\n",
        "IMAGE_DATA_ZIP = \"image_data.json.zip\"\n",
        "\n",
        "SEED = 42 #for reproducibility\n",
        "BATCH_EMB_BUILD = 24  # building embeddings batch (lower to avoid OOM)\n",
        "TRAIN_BATCH = 8\n",
        "VAL_BATCH = 16\n",
        "EPOCHS = 4\n",
        "\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "metadata": {
        "id": "AJe2sEui_ycF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset class (Embeddings)\n",
        "class EmbeddingDataset(Dataset):\n",
        "\tdef __init__(self, path):\n",
        "\t\tdataset_metadata_dict = torch.load(path)\n",
        "\t\tself.text = dataset_metadata_dict[\"text\"]\n",
        "\t\tself.img  = dataset_metadata_dict[\"img\"]\n",
        "\t\tself.text_mask = dataset_metadata_dict.get(\"text_mask\", torch.ones(self.text.shape[0], self.text.shape[1], dtype=torch.long))\n",
        "\t\tself.img_mask = dataset_metadata_dict.get(\"img_mask\", torch.ones(self.img.shape[0], self.img.shape[1], dtype=torch.long))\n",
        "\t\tself.labels = dataset_metadata_dict[\"labels\"]\n",
        "\n",
        "\tdef __len__(self):\n",
        "\t\treturn len(self.labels)\n",
        "\n",
        "\tdef __getitem__(self, idx):\n",
        "\t\treturn {\n",
        "\t\t\t\"text_embedding\": self.text[idx],   # (Nt, D_txt)\n",
        "\t\t\t\"image_embedding\": self.img[idx],   # (Ni, D_img)\n",
        "\t\t\t\"text_mask\": self.text_mask[idx],\n",
        "\t\t\t\"image_mask\": self.img_mask[idx],\n",
        "\t\t\t\"label\": self.labels[idx]\n",
        "\t\t}"
      ],
      "metadata": {
        "id": "pWX8d4MT_vRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Defining Cross Attention Fusion mechanism using text and image embeddings**"
      ],
      "metadata": {
        "id": "tpn7y1xBBOm0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cross Attention Fusion Model\n",
        "class CrossAttentionBlock(nn.Module):\n",
        "\tdef __init__(self, d_model, nhead=8, dim_ff=2048, dropout=0.1):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.mha = nn.MultiheadAttention(d_model, nhead, batch_first=True)\n",
        "\t\tself.norm1 = nn.LayerNorm(d_model)\n",
        "\t\tself.ff = nn.Sequential(\n",
        "\t\t\tnn.Linear(d_model, dim_ff),\n",
        "\t\t\tnn.GELU(),\n",
        "\t\t\tnn.Dropout(dropout),\n",
        "\t\t\tnn.Linear(dim_ff, d_model)\n",
        "\t\t)\n",
        "\t\tself.norm2 = nn.LayerNorm(d_model)\n",
        "\n",
        "\tdef forward(self, q, kv, kv_key_padding_mask=None):\n",
        "\t\tattn_out, _ = self.mha(query=q, key=kv, value=kv, key_padding_mask=kv_key_padding_mask)\n",
        "\t\tq = self.norm1(q + attn_out)\n",
        "\t\tq2 = self.ff(q)\n",
        "\t\treturn self.norm2(q + q2)\n",
        "\n",
        "class CrossAttentionFusionNetwork(nn.Module):\n",
        "\tdef __init__(self, d_img=768, d_txt=768, d=512, n_heads=8, num_answers=1000, num_cross_layers=2, dropout=0.3):\n",
        "\t\tsuper().__init__()\n",
        "\t\tself.P_img = nn.Linear(d_img, d)\n",
        "\t\tself.P_txt = nn.Linear(d_txt, d)\n",
        "\t\tself.cross_blocks = nn.ModuleList([CrossAttentionBlock(d_model=d, nhead=n_heads) for _ in range(num_cross_layers)])\n",
        "\t\tself.fc1 = nn.Linear(d, 256)\n",
        "\t\tself.bn1 = nn.BatchNorm1d(256)\n",
        "\t\tself.classifier = nn.Linear(256, num_answers)\n",
        "\t\tself.relu = nn.ReLU()\n",
        "\t\tself.dropout = nn.Dropout(dropout)\n",
        "\n",
        "\tdef forward(self, image_embedding, text_embedding, image_mask=None, text_mask=None):\n",
        "\t\tI = self.P_img(image_embedding)  # [B, Ni, d]\n",
        "\t\tT = self.P_txt(text_embedding)   # [B, Nt, d]\n",
        "\t\tkv_mask = None\n",
        "\t\tif image_mask is not None:\n",
        "\t\t\tkv_mask = (image_mask == 0)\n",
        "\t\tTq = T\n",
        "\t\tfor blk in self.cross_blocks:\n",
        "\t\t\tTq = blk(Tq, I, kv_key_padding_mask=kv_mask)\n",
        "\t\tpooled = Tq[:, 0, :]  # assumes CLS at 0\n",
        "\t\tx = self.relu(self.fc1(pooled))\n",
        "\t\tx = self.bn1(x)\n",
        "\t\tx = self.dropout(x)\n",
        "\t\tlogits = self.classifier(x)\n",
        "\t\treturn logits"
      ],
      "metadata": {
        "id": "qGu8H38p_qGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Utilities for Data preparation**"
      ],
      "metadata": {
        "id": "hyWnY08dBF5j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Utility function to download and unzip the JSON\n",
        "def download_and_unzip_vg_jsons(target_dir=ROOT, timeout=60):\n",
        "\tos.makedirs(target_dir, exist_ok=True) # if root dir does't exist, create directory\n",
        "\n",
        "\tqa_zip_url = f\"{VISUAL_GENOME_BASE}/{QUESTION_ANS_ZIP}\" #url to the QA pairs (zip file)\n",
        "\timg_meta_zip_url = f\"{VISUAL_GENOME_BASE}/{IMAGE_DATA_ZIP}\" #url to the image metadata (zip file)\n",
        "\tqa_zip_path = os.path.join(target_dir, QUESTION_ANS_ZIP) # local target to the downloaded QA pirs (zip file)\n",
        "\timg_zip_path = os.path.join(target_dir, IMAGE_DATA_ZIP) # local target to the downloaded images metadata (zip file)\n",
        "\n",
        "\t# Helper function to download a file from a URL\n",
        "\tdef fetch(url, out_path):\n",
        "\t\tprint(\"Downloading ZIP file from: \", url)\n",
        "\t\tfetch_request = requests.get(url, timeout=timeout, stream=True) # send a GET request to download the file in streaming mode\n",
        "\n",
        "\t\tif fetch_request.status_code != 200:\n",
        "\t\t\traise RuntimeError(f\"Failed to download ZIP file at {url} with status {fetch_request.status_code}\")\n",
        "\n",
        "\t\twith open(out_path, \"wb\") as f:\n",
        "\t\t\tfor chunk in fetch_request.iter_content(chunk_size=8192): # write the file in chunks (to avoid loading everything into memory)\n",
        "\t\t\t\tif chunk:\n",
        "\t\t\t\t\tf.write(chunk) # only write on non-empty chunks\n",
        "\t\tprint(\"Saved ZIP file in path: \", out_path)\n",
        "\n",
        "\tif not os.path.exists(qa_zip_path):\n",
        "\t\tfetch(qa_zip_url, qa_zip_path)\n",
        "\telse:\n",
        "\t\tprint(\"Path to the QA pairs already exists in \", qa_zip_path, \"\\n Using QA pairs from path\")\n",
        "\n",
        "\tif not os.path.exists(img_zip_path):\n",
        "\t\tfetch(img_meta_zip_url, img_zip_path)\n",
        "\telse:\n",
        "\t\tprint(\"Path to the images metadata already exists in \", img_zip_path, \"\\n Using images metadata from path\")\n",
        "\n",
        "\t# Loop over both downloaded ZIP files to extract their contents\n",
        "\tfor z in (qa_zip_path, img_zip_path):\n",
        "\t\tprint(\"Unzipping ZIP file: \", z)\n",
        "\t\twith ZipFile(z, 'r') as zip_ref:\n",
        "\t\t\tzip_ref.extractall(target_dir)\n",
        "\n",
        "\tprint(\"Download & unzip done, JSON files saved in \", target_dir)\n",
        "\n",
        "# Function to load Visual Genome QA from local JSON into a flat pandas dataframe\n",
        "def load_vg_qa_local(json_path, limit=2000):\n",
        "\tif not os.path.exists(json_path):\n",
        "\t\traise FileNotFoundError(f\"File not found: {json_path}\")\n",
        "\n",
        "\twith open(json_path, \"r\", encoding=\"utf-8\") as f:\n",
        "\t\tdata = json.load(f)\n",
        "\n",
        "\trecords = []\n",
        "\tfor item in data:\n",
        "\t\timage_id = item.get(\"id\", None)\n",
        "\t\tfor qa_pair in item.get(\"qas\", []):\n",
        "\t\t\trecords.append({\n",
        "\t\t\t\t\"image_id\": image_id,\n",
        "\t\t\t\t\"question\": str(qa_pair[\"question\"]).strip(),\n",
        "\t\t\t\t\"answer\": str(qa_pair[\"answer\"]).strip().lower()\n",
        "\t\t\t})\n",
        "\t\t\tif limit and len(records) >= limit:\n",
        "        break\n",
        "\t\tif limit and len(records) >= limit:\n",
        "      break\n",
        "\n",
        "\tdf = pd.DataFrame(records)\n",
        "\tprint(f\"Loaded {len(df)} QA pairs from Visual Genome\")\n",
        "\treturn df\n",
        "\n",
        "# Load image metadata mapping\n",
        "def load_vg_image_data_from_local(local_dir=ROOT):\n",
        "\tlocal_img_meta = None\n",
        "\tp = os.path.join(local_dir, \"image_data.json\")\n",
        "\n",
        "\tif os.path.exists(p):\n",
        "\t\tlocal_img_meta = p\n",
        "\n",
        "\tif local_img_meta is None:\n",
        "\t\tprint(\"No image_data.json found in\", local_dir, \", returing empty metadata\")\n",
        "\t\treturn {}\n",
        "\n",
        "\twith open(local_img_meta, \"r\", encoding=\"utf-8\") as f:\n",
        "\t\tdata = json.load(f)\n",
        "\n",
        "\tmapping = {}\n",
        "\tfor rec in data:\n",
        "\t\timage_id = int(rec.get(\"image_id\") or rec.get(\"id\"))\n",
        "\t\tmapping[image_id] = {\"url\": rec.get(\"url\"), \"width\": rec.get(\"width\"), \"height\": rec.get(\"height\")}\n",
        "\n",
        "\tprint(\"Loaded image metadata for\", len(mapping), \"images\")\n",
        "\treturn mapping\n",
        "\n",
        "# Pre-download images for a DataFrame (subset)\n",
        "def predownload_images_for_df(df, image_meta_map, out_dir=IMG_DIR, max_images=1000):\n",
        "\tos.makedirs(out_dir, exist_ok=True)\n",
        "\timage_ids = list(dict.fromkeys(df[\"image_id\"].tolist()))\n",
        "\tcnt = 0\n",
        "\n",
        "\tfor img_id in tqdm(image_ids, desc=\"predownloading images\"):\n",
        "\t\tif cnt >= max_images:\n",
        "\t\t\tbreak\n",
        "\n",
        "\t\tmeta = image_meta_map.get(int(img_id))\n",
        "\n",
        "\t\tif not meta:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\turl = meta.get(\"url\")\n",
        "\t\tif not url:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tout_path = os.path.join(out_dir, f\"{int(img_id)}.jpg\")\n",
        "\t\tif os.path.exists(out_path):\n",
        "\t\t\tcnt += 1\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\ttry:\n",
        "\t\t\tr = requests.get(url, timeout=10)\n",
        "\n",
        "\t\t\tif r.status_code == 200:\n",
        "\t\t\t\twith open(out_path, \"wb\") as f:\n",
        "\t\t\t\t\tf.write(r.content)\n",
        "\t\t\t\tcnt += 1\n",
        "\t\texcept Exception:\n",
        "\t\t\tcontinue\n",
        "\tprint(\"Downloaded\", cnt, \"images to\", out_dir)"
      ],
      "metadata": {
        "id": "g6tae-i9_fFi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Feature Engineering and Extraction (building and saving text embeddings and image embeddings from text and image backbones)**"
      ],
      "metadata": {
        "id": "HRGwV5bLAaq3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ---------------------------\n",
        "# Build sequence-level embeddings (BERT tokens and ViT patch tokens)\n",
        "# Tolerant: uses local IMG_DIR images first; if missing, tries image_meta_map URL (download)\n",
        "# ---------------------------\n",
        "def build_and_save_embeddings(df, tokenizer, img_preprocessor, text_encoder, img_encoder, image_meta_map, out_path,\n",
        "\t\t\t\t\t\t\t  local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD, max_text_len=64):\n",
        "\t\"\"\"\n",
        "\tBuilds and saves sequence-level embeddings with a fixed text token length.\n",
        "\n",
        "\t- text_seq: (N, max_text_len, D_txt)\n",
        "\t- text_mask: (N, max_text_len)\n",
        "\t- img_seq:  (N, Ni, D_img)  (Ni fixed for ViT)\n",
        "\t- img_mask: (N, Ni)\n",
        "\t\"\"\"\n",
        "\ttext_encoder.eval()\n",
        "\timg_encoder.eval()\n",
        "\ttext_encoder.to(device)\n",
        "\timg_encoder.to(device)\n",
        "\tos.makedirs(local_image_dir, exist_ok=True)\n",
        "\n",
        "\tall_text_seq = []\n",
        "\tall_img_seq = []\n",
        "\tall_text_masks = []\n",
        "\tall_img_masks = []\n",
        "\tall_labels = []\n",
        "\tdropped = 0\n",
        "\n",
        "\tfor i in tqdm(range(0, len(df), batch_size), desc=f\"Building {out_path}\"):\n",
        "\t\tbatch = df.iloc[i:i+batch_size]\n",
        "\t\tqs = batch[\"question\"].tolist()\n",
        "\t\timage_ids = batch[\"image_id\"].tolist()\n",
        "\n",
        "\t\timgs = []\n",
        "\t\tvalid_indices = []\n",
        "\t\tfor idx, img_id in enumerate(image_ids):\n",
        "\t\t\tlocal_path = os.path.join(local_image_dir, f\"{int(img_id)}.jpg\")\n",
        "\t\t\timg_path = None\n",
        "\t\t\tif os.path.exists(local_path):\n",
        "\t\t\t\timg_path = local_path\n",
        "\t\t\telse:\n",
        "\t\t\t\tmeta = image_meta_map.get(int(img_id), {})\n",
        "\t\t\t\turl = meta.get(\"url\")\n",
        "\t\t\t\tif url:\n",
        "\t\t\t\t\ttry:\n",
        "\t\t\t\t\t\t# try to download on demand\n",
        "\t\t\t\t\t\tr = requests.get(url, timeout=10)\n",
        "\t\t\t\t\t\tif r.status_code == 200:\n",
        "\t\t\t\t\t\t\twith open(local_path, \"wb\") as f:\n",
        "\t\t\t\t\t\t\t\tf.write(r.content)\n",
        "\t\t\t\t\t\t\timg_path = local_path\n",
        "\t\t\t\t\texcept Exception:\n",
        "\t\t\t\t\t\timg_path = None\n",
        "\t\t\tif not img_path or not os.path.exists(img_path):\n",
        "\t\t\t\tdropped += 1\n",
        "\t\t\t\tcontinue\n",
        "\t\t\ttry:\n",
        "\t\t\t\timg = Image.open(img_path).convert(\"RGB\")\n",
        "\t\t\t\timgs.append(img)\n",
        "\t\t\t\tvalid_indices.append(idx)\n",
        "\t\t\texcept Exception:\n",
        "\t\t\t\tdropped += 1\n",
        "\t\t\t\tcontinue\n",
        "\n",
        "\t\tif len(imgs) == 0:\n",
        "\t\t\tcontinue\n",
        "\n",
        "\t\tbatch_qs = [qs[k] for k in valid_indices]\n",
        "\t\tbatch_labels = batch[\"label\"].values[valid_indices].tolist()\n",
        "\n",
        "\t\twith torch.no_grad():\n",
        "\t\t\t# Tokenize with fixed max length so all batches have same Nt\n",
        "\t\t\ttokenized_qs = tokenizer(batch_qs, padding=\"max_length\", truncation=True, max_length=max_text_len, return_tensors=\"pt\")\n",
        "\t\t\ttokenized_qs = {k: v.to(device) for k, v in tokenized_qs.items()}\n",
        "\t\t\ttext_outputs = text_encoder(**tokenized_qs)\n",
        "\t\t\t# text_outputs.last_hidden_state -> (B, max_text_len, D_txt)\n",
        "\t\t\ttext_seq = text_outputs.last_hidden_state.detach().cpu()\n",
        "\t\t\ttext_mask = tokenized_qs[\"attention_mask\"].detach().cpu()  # (B, max_text_len)\n",
        "\n",
        "\t\t\t# Images -> ViT patch tokens (Ni fixed)\n",
        "\t\t\timg_inputs = img_preprocessor(images=imgs, return_tensors=\"pt\")\n",
        "\t\t\timg_inputs = {k: v.to(device) for k, v in img_inputs.items()}\n",
        "\t\t\timg_outputs = img_encoder(**img_inputs)\n",
        "\t\t\timg_seq = img_outputs.last_hidden_state.detach().cpu()  # (B, Ni, D_img)\n",
        "\t\t\timg_mask = torch.ones(img_seq.shape[0], img_seq.shape[1], dtype=torch.long)\n",
        "\n",
        "\t\tall_text_seq.append(text_seq)\n",
        "\t\tall_img_seq.append(img_seq)\n",
        "\t\tall_text_masks.append(text_mask)\n",
        "\t\tall_img_masks.append(img_mask)\n",
        "\t\tall_labels.append(torch.tensor(batch_labels, dtype=torch.long))\n",
        "\n",
        "\tif len(all_labels) == 0:\n",
        "\t\traise RuntimeError(\"No embeddings were created (no valid images found). Check local images or image_meta_map URLs.\")\n",
        "\n",
        "\t# Now concatenation will succeed because every text_seq has shape (B, max_text_len, D)\n",
        "\ttext_seq = torch.cat(all_text_seq, dim=0)\n",
        "\timg_seq = torch.cat(all_img_seq, dim=0)\n",
        "\ttext_masks = torch.cat(all_text_masks, dim=0)\n",
        "\timg_masks = torch.cat(all_img_masks, dim=0)\n",
        "\tlabels = torch.cat(all_labels, dim=0)\n",
        "\n",
        "\ttorch.save({\"text\": text_seq, \"img\": img_seq, \"text_mask\": text_masks, \"img_mask\": img_masks, \"labels\": labels}, out_path)\n",
        "\tprint(\"Saved embeddings:\", out_path, \"Dropped samples during build:\", dropped)"
      ],
      "metadata": {
        "id": "nEj-yvFh_YWS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Data Preparation (Building Dataframe for Feature Engineering and Extraction)**"
      ],
      "metadata": {
        "id": "IKY8ohQg_8ki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_vg_dataframe(local_dir=ROOT, limit=None):\n",
        "\tqa_json = os.path.join(local_dir, \"question_answers.json\")\n",
        "\n",
        "\tif not os.path.exists(qa_json):\n",
        "\t\traise FileNotFoundError(f\"Please put 'question_answers.json' into {local_dir} or run download_and_unzip_vg_jsons().\")\n",
        "\n",
        "\tprint(\"Loading QA from\", qa_json)\n",
        "\tdf = load_vg_qa_local(qa_json)\n",
        "\tdf[\"question\"] = df[\"question\"].astype(str)\n",
        "\tdf[\"answer\"] = df[\"answer\"].astype(str).str.lower().str.strip()\n",
        "\tdf = df[df[\"question\"].str.len() > 0]\n",
        "\n",
        "\tif limit:\n",
        "\t\tdf = df.sample(n=min(limit, len(df)), random_state=SEED).reset_index(drop=True)\n",
        "\telse:\n",
        "\t\tdf = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
        "\n",
        "\tn = len(df)\n",
        "\tn_train = int(0.8 * n)\n",
        "\tn_val = int(0.1 * n)\n",
        "\ttrain_df = df.iloc[:n_train].reset_index(drop=True)\n",
        "\tval_df = df.iloc[n_train:n_train + n_val].reset_index(drop=True)\n",
        "\ttest_df = df.iloc[n_train + n_val:].reset_index(drop=True)\n",
        "\tprint(\"Split sizes -> train:\", len(train_df), \"val:\", len(val_df), \"test:\", len(test_df))\n",
        "\n",
        "\treturn train_df, val_df, test_df\n",
        "\n",
        "def map_label(ans):\n",
        "\treturn ans_to_labels.get(ans, -1)"
      ],
      "metadata": {
        "id": "EWuAk5At-_zb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Evaluate and Train the Model**"
      ],
      "metadata": {
        "id": "2llSqdtr-xTW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, val_dataloader, criterion, device=DEVICE):\n",
        "\tmodel.eval()\n",
        "\ttotal_val_loss = 0.0\n",
        "\tpredictions = []\n",
        "\ttrue_vals = []\n",
        "\tconf = []\n",
        "\n",
        "\twith torch.no_grad():\n",
        "\t\tfor batch in val_dataloader:\n",
        "\t\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t'image_embedding': batch['image_embedding'],\n",
        "\t\t\t\t'text_embedding': batch['text_embedding'],\n",
        "\t\t\t\t'image_mask': batch.get('image_mask', None),\n",
        "\t\t\t\t'text_mask': batch.get('text_mask', None)\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\toutputs = model(**inputs)\n",
        "\t\t\tlabels = batch['label']\n",
        "\t\t\tloss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "\t\t\ttotal_val_loss += loss.item()\n",
        "\t\t\tprobs = torch.max(outputs.softmax(dim=1), dim=-1)[0].detach().cpu().numpy()\n",
        "\t\t\toutputs = outputs.argmax(-1)\n",
        "\t\t\tpredictions.append(outputs.detach().cpu().numpy())\n",
        "\t\t\ttrue_vals.append(labels.cpu().numpy())\n",
        "\t\t\tconf.append(probs)\n",
        "\n",
        "\tloss_val_avg = total_val_loss / len(val_dataloader)\n",
        "\tpredictions = np.concatenate(predictions, axis=0)\n",
        "\ttrue_vals = np.concatenate(true_vals, axis=0)\n",
        "\tconf = np.concatenate(conf, axis=0)\n",
        "\n",
        "\treturn loss_val_avg, predictions, true_vals, conf\n",
        "\n",
        "def train_loop(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epochs=EPOCHS, device=DEVICE):\n",
        "\tbest_val_acc = 0.0\n",
        "\n",
        "\tfor epoch in range(1, epochs+1):\n",
        "\t\tmodel.train()\n",
        "\t\ttotal_train_loss = 0.0\n",
        "\t\ttrain_predictions = []\n",
        "\t\ttrain_true_vals = []\n",
        "\n",
        "\t\tfor batch in tqdm(train_dataloader, desc=f\"Train epoch {epoch}\"):\n",
        "\t\t\tbatch = {k: v.to(device) for k, v in batch.items()}\n",
        "\t\t\tinputs = {\n",
        "\t\t\t\t'image_embedding': batch['image_embedding'],\n",
        "\t\t\t\t'text_embedding': batch['text_embedding'],\n",
        "\t\t\t\t'image_mask': batch.get('image_mask', None),\n",
        "\t\t\t\t'text_mask': batch.get('text_mask', None)\n",
        "\t\t\t}\n",
        "\n",
        "\t\t\tlabels = batch['label']\n",
        "\t\t\toutputs = model(**inputs)\n",
        "\t\t\tloss = criterion(outputs.view(-1, outputs.shape[-1]), labels.view(-1))\n",
        "\t\t\toptimizer.zero_grad()\n",
        "\t\t\tloss.backward()\n",
        "\t\t\ttorch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\t\t\toptimizer.step()\n",
        "\t\t\tscheduler.step()\n",
        "\t\t\ttotal_train_loss += loss.item()\n",
        "\t\t\ttrain_predictions.append(outputs.argmax(-1).detach().cpu().numpy())\n",
        "\t\t\ttrain_true_vals.append(labels.cpu().numpy())\n",
        "\n",
        "\t\ttrain_preds = np.concatenate(train_predictions, axis=0)\n",
        "\t\ttrain_trues = np.concatenate(train_true_vals, axis=0)\n",
        "\t\ttrain_acc = accuracy_score(train_preds, train_trues)\n",
        "\t\tval_loss, val_preds, val_trues, _ = evaluate(model, val_dataloader, criterion, device=device)\n",
        "\t\tval_acc = accuracy_score(val_preds, val_trues)\n",
        "\t\tprint(f\"Epoch {epoch}: train_loss={total_train_loss/len(train_dataloader):.4f}, train_acc={train_acc:.4f}, val_loss={val_loss:.4f}, val_acc={val_acc:.4f}\")\n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "\t\t\tbest_val_acc = val_acc\n",
        "\t\t\ttorch.save(model.state_dict(), \"best_model_vg_fusion.pth\")\n",
        "\t\t\tprint(\"Saved best model with val_acc:\", best_val_acc)\n",
        "\n",
        "\treturn best_val_acc\n"
      ],
      "metadata": {
        "id": "EX7m5jG6-lMD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**VQA Pipeline**"
      ],
      "metadata": {
        "id": "teg9tg46-bQ5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 547
        },
        "id": "MbuDZRCqfoUw",
        "outputId": "b5a69501-7dd5-4880-e6fb-8be30eb8616a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch_xla/experimental/gru.py:113: SyntaxWarning: invalid escape sequence '\\_'\n",
            "  * **h_n**: tensor of shape :math:`(D * \\text{num\\_layers}, H_{out})` or\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading Visual Genome JSONs (if not present)...\n",
            "Downloading ZIP file from:  https://homes.cs.washington.edu/~ranjay/visualgenome/data/dataset/question_answers.json.zip\n",
            "Saved ZIP file in path:  ./visual_genome_data/question_answers.json.zip\n",
            "Downloading ZIP file from:  https://homes.cs.washington.edu/~ranjay/visualgenome/data/dataset/image_data.json.zip\n",
            "Saved ZIP file in path:  ./visual_genome_data/image_data.json.zip\n",
            "Unzipping ZIP file:  ./visual_genome_data/question_answers.json.zip\n",
            "Unzipping ZIP file:  ./visual_genome_data/image_data.json.zip\n",
            "Download & unzip done, JSON files saved in  ./visual_genome_data\n",
            "Loading QA from ./visual_genome_data/question_answers.json\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1718564595.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    530\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[0;31m# Small default run for quick experiments - adjust params to scale up\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 532\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit_examples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_images_to_download\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_jsons\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1718564595.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m(limit_examples, top_k, max_images_to_download, download_jsons)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m         \u001b[0;31m# 2) build dataframes (limit controls experiment size)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m         \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_vg_dataframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlocal_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mROOT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlimit_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m         \u001b[0;31m# 3) load image meta mapping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1718564595.py\u001b[0m in \u001b[0;36mbuild_vg_dataframe\u001b[0;34m(local_dir, limit)\u001b[0m\n\u001b[1;32m    429\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Please put 'question_answers.json' into {local_dir} or run download_and_unzip_vg_jsons().\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading QA from\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mqa_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_vg_qa_local\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqa_json\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m     \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1718564595.py\u001b[0m in \u001b[0;36mload_vg_qa_local\u001b[0;34m(json_path, limit)\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Loaded {len(df)} QA pairs from Visual Genome\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m                         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                     arrays, columns, index = nested_data_to_arrays(\n\u001b[0m\u001b[1;32m    852\u001b[0m                         \u001b[0;31m# error: Argument 3 to \"nested_data_to_arrays\" has incompatible\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                         \u001b[0;31m# type \"Optional[Collection[Any]]\"; expected \"Optional[Index]\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mnested_data_to_arrays\u001b[0;34m(data, columns, index, dtype)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fields\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m     \u001b[0marrays\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36mto_arrays\u001b[0;34m(data, columns, dtype)\u001b[0m\n\u001b[1;32m    835\u001b[0m         \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    836\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMapping\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 837\u001b[0;31m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_dict_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    838\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    839\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_list_of_series_to_arrays\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m_list_of_dict_to_arrays\u001b[0;34m(data, columns)\u001b[0m\n\u001b[1;32m    915\u001b[0m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 917\u001b[0;31m         \u001b[0mpre_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    918\u001b[0m         \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mlib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.fast_unique_multiple_list_gen\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/core/internals/construction.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    913\u001b[0m     \"\"\"\n\u001b[1;32m    914\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m         \u001b[0msort\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m         \u001b[0mpre_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfast_unique_multiple_list_gen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "def vqa_pipeline(limit_examples=2000, top_k=1000, max_images_to_download=1000, download_jsons=True): #limiting to only 2000 examples for experimenting (default), None for full dataset (appox 108000 examples)\n",
        "\ttry:\n",
        "    #--------------------------------------------------Data Preparation------------------------------------------------------------------\n",
        "\t\t# Downloading JSONS for extracting QA pairs and images metadata (set to False, if already downloaded locally)\n",
        "\t\tif download_jsons:\n",
        "\t\t\tprint(\"Downloading Visual Genome JSONs\")\n",
        "\t\t\tdownload_and_unzip_vg_jsons(ROOT)\n",
        "\n",
        "\t\t# Building dataframes (based on limit size)\n",
        "\t\ttrain_df, val_df, test_df = build_vg_dataframe(local_dir=ROOT, limit=limit_examples)\n",
        "\n",
        "\t\t# Load image meta mapping\n",
        "\t\timage_meta_map = load_vg_image_data_from_local(local_dir=ROOT)\n",
        "\n",
        "\t\t# Build top-K answer vocabulary (for classifying answers)\n",
        "\t\tall_answers = pd.concat([train_df[\"answer\"], val_df[\"answer\"], test_df[\"answer\"]])\n",
        "\t\tans_counts = Counter(all_answers.tolist())\n",
        "\t\tmost_common = [a for a, _ in ans_counts.most_common(top_k)]\n",
        "\t\tans_to_labels = {a: i for i, a in enumerate(most_common)}\n",
        "\t\tlabel_to_ans = {i: a for a, i in ans_to_labels.items()}\n",
        "\t\tprint(\"Top-K answer vocab size:\", len(ans_to_labels))\n",
        "\n",
        "    train_df[\"label\"] = train_df[\"answer\"].apply(map_label)\n",
        "\t\tval_df[\"label\"] = val_df[\"answer\"].apply(map_label)\n",
        "\t\ttest_df[\"label\"] = test_df[\"answer\"].apply(map_label)\n",
        "\t\t# drop -1s\n",
        "\t\ttrain_df = train_df[train_df[\"label\"] >= 0].reset_index(drop=True)\n",
        "\t\tval_df = val_df[val_df[\"label\"] >= 0].reset_index(drop=True)\n",
        "\t\ttest_df = test_df[test_df[\"label\"] >= 0].reset_index(drop=True)\n",
        "\t\tprint(\"After mapping to top-K -> train:\", len(train_df), \"val:\", len(val_df), \"test:\", len(test_df))\n",
        "\n",
        "\t\t# Pre-download images subset for speed\n",
        "\t\tprint(\"Pre-downloading up to\", max_images_to_download, \"images for training/val/test\")\n",
        "\t\tcombined_df = pd.concat([train_df, val_df, test_df]).reset_index(drop=True)\n",
        "\t\tpredownload_images_for_df(combined_df, image_meta_map, out_dir=IMG_DIR, max_images=max_images_to_download)\n",
        "\n",
        "\n",
        "    #--------------------------------------------------Feature Engineering------------------------------------------------------------------\n",
        "\t\t# Load backbones (frozen)\n",
        "\t\ttokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\") # loads BERT's tokenizer to tokenize the question strings into many lowercased tokens and convert them into token IDs that BERT understands\n",
        "\t\ttext_encoder_bert = AutoModel.from_pretrained(\"bert-base-uncased\") # loads a pre-trained BERT model used for converting the token IDs and produces dense vector embeddings for each token\n",
        "\t\tfor p in text_encoder_bert.parameters(): # leaving the parameters (tensors) of the BERT model unchanged (frozen) (to avoid re-training it when the entire fusion network learns)\n",
        "\t\t\tp.requires_grad = False # No need to compute gradients for each tensor during backpropagation\n",
        "\n",
        "\t\timg_preprocessor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\") # loads ViT’s feature extractor that normalizes & resizes images to 224×224 patches\n",
        "\t\timg_encoder_vit = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\") #loads the ViT model\n",
        "\t\tfor p in img_encoder_vit.parameters(): # leaving the parameters (tensors) of the ViT model unchanged (frozen) (to avoid re-training it when the entire fusion network learns)\n",
        "\t\t\tp.requires_grad = False ## No need to compute gradients for each tensor during backpropagation\n",
        "\n",
        "\t\t# Build embeddings files (sequence-level)\n",
        "\t\tif not os.path.exists(TRAIN_EMB):\n",
        "\t\t\tbuild_and_save_embeddings(train_df, tokenizer, img_preprocessor, text_encoder_bert, img_encoder_vit, image_meta_map, TRAIN_EMB, local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD)\n",
        "\t\tif not os.path.exists(VAL_EMB):\n",
        "\t\t\tbuild_and_save_embeddings(val_df, tokenizer, img_preprocessor, text_encoder_bert, img_encoder_vit, image_meta_map, VAL_EMB, local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD)\n",
        "\t\tif not os.path.exists(TEST_EMB):\n",
        "\t\t\tbuild_and_save_embeddings(test_df, tokenizer, img_preprocessor, text_encoder_bert, img_encoder_vit, image_meta_map, TEST_EMB, local_image_dir=IMG_DIR, device=DEVICE, batch_size=BATCH_EMB_BUILD)\n",
        "\n",
        "    #-----------------------------------------------------Model Initialization-----------------------------------------------------------------\n",
        "\t\t# Datasets / Dataloaders\n",
        "\t\ttrain_dataset = EmbeddingDataset(TRAIN_EMB)\n",
        "\t\tval_dataset = EmbeddingDataset(VAL_EMB)\n",
        "\t\ttest_dataset = EmbeddingDataset(TEST_EMB)\n",
        "\t\ttrain_dataloader = DataLoader(train_dataset, batch_size=TRAIN_BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\t\tval_dataloader = DataLoader(val_dataset, batch_size=VAL_BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\t\ttest_dataloader = DataLoader(test_dataset, batch_size=VAL_BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "\t\t# Model and Training setup\n",
        "\t\tnum_answers_total = len(ans_to_labels)\n",
        "\t\tmodel = CrossAttentionFusionNetwork(d_img=768, d_txt=768, d=512, n_heads=8, num_answers=num_answers_total, num_cross_layers=2, dropout=0.3)\n",
        "\t\tmodel.to(DEVICE)\n",
        "\t\tcriterion = nn.CrossEntropyLoss()\n",
        "\t\ttrain_steps = len(train_dataloader) * EPOCHS\n",
        "\t\twarm_steps = max(1, int(train_steps * 0.1))\n",
        "\t\toptimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=1e-5, eps=1e-8)\n",
        "\t\tscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=warm_steps, num_training_steps=train_steps)\n",
        "\n",
        "    #-----------------------------------------------------Model Training + Testing-----------------------------------------------------------------\n",
        "\t\t# Train\n",
        "\t\tbest_val = train_loop(model, train_dataloader, val_dataloader, criterion, optimizer, scheduler, epochs=EPOCHS, device=DEVICE)\n",
        "\t\tprint(\"Best val acc:\", best_val)\n",
        "\n",
        "\t\t# Test\n",
        "\t\tmodel.load_state_dict(torch.load(\"best_model_vg_fusion.pth\", map_location=DEVICE))\n",
        "\t\ttest_loss, preds, truths, conf = evaluate(model, test_dataloader, criterion, device=DEVICE)\n",
        "\t\ttest_acc, test_prec, test_recall, test_f1 = accuracy_score(preds, truths), precision_score(preds, truths), recall_score(preds, truths), f1_score(preds, truths)\n",
        "\t\tprint(\"Test loss:\", test_loss)\n",
        "\t\tprint(\"Test performance:\")\n",
        "\t\tprint(f\"Accuracy: {test_acc:.3f}, Precision: {test_prec:.3f}, Recall: {test_recall:.3f}, F1 Score: {test_f1:.3f}\")\n",
        "\n",
        "\texcept Exception as e:\n",
        "\t\tprint(\"Error in VQA pipeline due to error:\", e)\n",
        "\t\tprint(traceback.format_exc())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Execute the pipeline**"
      ],
      "metadata": {
        "id": "ZI1ze6PT-BD0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "\t# TODO: adjust params to scale up training\n",
        "\tvqa_pipeline(limit_examples=2000, top_k=1000, max_images_to_download=1000, download_jsons=True) #small params for quick experiments"
      ],
      "metadata": {
        "id": "sE_VOJTU-JLX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}